{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "This note book explains the different steps in the machine learning model.For the trigger model we used a Regression model. First the model is trained on the full dataset to obtain the optimal features followed by hyper parameter tunning and model performance estimate using Nested Cross Validation.\n",
    "\n",
    "* Nested Cross Validation for\n",
    "    * Feature selection \n",
    "    * hyper parameter tunning \n",
    "* Performance metrics\n",
    "* Baseline Models\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from xgboost import XGBClassifier\n",
    "import os\n",
    "from sklearn.feature_selection import RFECV\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    StratifiedKFold,\n",
    "    KFold,\n",
    ")\n",
    "from sklearn.metrics import f1_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "from numpy.lib.function_base import average\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import (\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    confusion_matrix,\n",
    "    make_scorer,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    StratifiedKFold,\n",
    "    KFold,\n",
    ")\n",
    "from sklearn.feature_selection import SelectKBest, SequentialFeatureSelector\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "import importlib\n",
    "import os\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest,\n",
    "    RFE,\n",
    "    mutual_info_regression,\n",
    "    f_regression,\n",
    "    mutual_info_classif,\n",
    ")\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "import xgboost as xgb\n",
    "import random\n",
    "import pickle\n",
    "import openpyxl\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import RFECV\n",
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import geopandas as gpd\n",
    "import random\n",
    "import importlib\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitting_train_test(df):\n",
    "\n",
    "    # To save the train and test sets\n",
    "    df_train_list = []\n",
    "    df_test_list = []\n",
    "\n",
    "    # List of typhoons that are to be used as a test set \n",
    "    typhoon=typhoons_with_impact_data\n",
    "\n",
    "    for typhoon in typhoons_with_impact_data:\n",
    "\n",
    "        df_train_list.append(df[df[\"typhoon\"] != typhoon])\n",
    "        df_test_list.append(df[df[\"typhoon\"] == typhoon])\n",
    "\n",
    "    return df_train_list, df_test_list\n",
    "\n",
    "\n",
    "def unweighted_random(y_train, y_test):\n",
    "    options = y_train.value_counts(normalize=True)\n",
    "    y_pred = random.choices(population=list(options.index), k=len(y_test))\n",
    "    return y_pred\n",
    "\n",
    "def weighted_random(y_train, y_test):\n",
    "    options = y_train.value_counts()\n",
    "    y_pred = random.choices(\n",
    "        population=list(options.index), weights=list(options.values), k=len(y_test)\n",
    "    )\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting directory\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = Path.cwd()\n",
    "\n",
    "wor_dir = current_dir / Path('../../')\n",
    "\n",
    "\n",
    "os.chdir(wor_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions\n",
    "from models.regression.rf_regression import (rf_regression_features,rf_regression_performance,)\n",
    "from models.regression.xgb_regression import (xgb_regression_features,xgb_regression_performance,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_input_data=pd.read_csv(\"./data/combined_input_data.csv\")\n",
    "\n",
    "typhoons_with_impact_data=['bopha2012', 'conson2010', 'durian2006', 'fengshen2008',\n",
    "       'fung-wong2014', 'goni2015', 'goni2020', 'hagupit2014','haima2016', 'haiyan2013', 'kalmaegi2014', 'kammuri2019',\n",
    "       'ketsana2009', 'koppu2015', 'krosa2013', 'lingling2014','mangkhut2018', 'mekkhala2015', 'melor2015', 'mujigae2015',\n",
    "       'nari2013', 'nesat2011', 'nock-ten2016', 'noul2015','rammasun2014', 'sarika2016', 'trami2013', 'usagi2013', 'utor2013',\n",
    "       'vamco2020']\n",
    "\n",
    "combined_input_data=combined_input_data[combined_input_data.typhoon.isin(typhoons_with_impact_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_input_data['year']=combined_input_data['typhoon'].apply(lambda x: int(x[-4:]))\n",
    "\n",
    "combined_input_data.rename(columns ={\"rainfall_Total\":\"HAZ_rainfall_Total\",\n",
    "                                     'rainfall_max_6h':'HAZ_rainfall_max_6h',\n",
    "                                     'rainfall_max_24h':'HAZ_rainfall_max_24h',\n",
    "                                     'v_max':'HAZ_v_max',\n",
    "                                     'dis_track_min':'HAZ_dis_track_min',\n",
    "                                     'perc_dmg':'DAM_perc_dmg',\n",
    "                                    'landslide_per':'GEN_landslide_per',\n",
    "                                    'stormsurge_per':'GEN_stormsurge_per',\n",
    "                                    'Bu_p_inSSA':'GEN_Bu_p_inSSA',\n",
    "                                    'Bu_p_LS':'GEN_Bu_p_LS',\n",
    "                                     'Red_per_LSbldg':'GEN_Red_per_LSbldg',\n",
    "                                    'Or_per_LSblg':'GEN_Or_per_LSblg',\n",
    "                                     'Yel_per_LSSAb':'GEN_Yel_per_LSSAb',\n",
    "                                    'RED_per_SSAbldg':'GEN_RED_per_SSAbldg',\n",
    "                                     'OR_per_SSAbldg':'GEN_OR_per_SSAbldg',\n",
    "                                    'Yellow_per_LSbl':'GEN_Yellow_per_LSbl',\n",
    "                                     'mean_slope':'TOP_mean_slope',\n",
    "                                    'mean_elevation_m':'TOP_mean_elevation_m',\n",
    "                                     'ruggedness_stdev':'TOP_ruggedness_stdev',\n",
    "                                    'mean_ruggedness':'TOP_mean_ruggedness',\n",
    "                                     'slope_stdev':'TOP_slope_stdev',\n",
    "                                     'poverty_perc':'VUL_poverty_perc',\n",
    "                                    'with_coast':'GEN_with_coast',\n",
    "                                     'coast_length':'GEN_coast_length',\n",
    "                                     'Housing Units':'VUL_Housing_Units',\n",
    "                                    'Strong Roof/Strong Wall':\"VUL_StrongRoof_StrongWall\",\n",
    "                                    'Strong Roof/Light Wall':'VUL_StrongRoof_LightWall',\n",
    "                                    'Strong Roof/Salvage Wall':'VUL_StrongRoof_SalvageWall',\n",
    "                                    'Light Roof/Strong Wall':'VUL_LightRoof_StrongWall',\n",
    "                                    'Light Roof/Light Wall':'VUL_LightRoof_LightWall',\n",
    "                                    'Light Roof/Salvage Wall':'VUL_LightRoof_SalvageWall',\n",
    "                                    'Salvaged Roof/Strong Wall':'VUL_SalvagedRoof_StrongWall',\n",
    "                                    'Salvaged Roof/Light Wall':'VUL_SalvagedRoof_LightWall',\n",
    "                                    'Salvaged Roof/Salvage Wall':'VUL_SalvagedRoof_SalvageWall',\n",
    "                                    'vulnerable_groups':'VUL_vulnerable_groups',\n",
    "                                    'pantawid_pamilya_beneficiary':'VUL_pantawid_pamilya_beneficiary'},inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "combined_input_data =combined_input_data.filter(['typhoon','HAZ_rainfall_Total', \n",
    "        'HAZ_rainfall_max_6h',\n",
    "        'HAZ_rainfall_max_24h',\n",
    "        'HAZ_v_max',\n",
    "        'HAZ_dis_track_min',\n",
    "        'GEN_landslide_per',\n",
    "        'GEN_stormsurge_per',\n",
    "        'GEN_Bu_p_inSSA', \n",
    "        'GEN_Bu_p_LS', \n",
    "        'GEN_Red_per_LSbldg',\n",
    "        'GEN_Or_per_LSblg', \n",
    "        'GEN_Yel_per_LSSAb', \n",
    "        'GEN_RED_per_SSAbldg',\n",
    "        'GEN_OR_per_SSAbldg',\n",
    "        'GEN_Yellow_per_LSbl',\n",
    "        'TOP_mean_slope',\n",
    "        'TOP_mean_elevation_m', \n",
    "        'TOP_ruggedness_stdev', \n",
    "        'TOP_mean_ruggedness',\n",
    "        'TOP_slope_stdev', \n",
    "        'VUL_poverty_perc',\n",
    "        'GEN_with_coast',\n",
    "        'GEN_coast_length', \n",
    "        'VUL_Housing_Units',\n",
    "        'VUL_StrongRoof_StrongWall', \n",
    "        'VUL_StrongRoof_LightWall',\n",
    "        'VUL_StrongRoof_SalvageWall', \n",
    "        'VUL_LightRoof_StrongWall',\n",
    "        'VUL_LightRoof_LightWall', \n",
    "        'VUL_LightRoof_SalvageWall',\n",
    "        'VUL_SalvagedRoof_StrongWall',\n",
    "        'VUL_SalvagedRoof_LightWall',\n",
    "        'VUL_SalvagedRoof_SalvageWall', \n",
    "        'VUL_vulnerable_groups',\n",
    "        'VUL_pantawid_pamilya_beneficiary', \n",
    "        'DAM_perc_dmg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features =['HAZ_rainfall_Total', \n",
    "        'HAZ_rainfall_max_6h',\n",
    "        'HAZ_rainfall_max_24h',\n",
    "        'HAZ_v_max',\n",
    "        'HAZ_dis_track_min',\n",
    "        'GEN_landslide_per',\n",
    "        'GEN_stormsurge_per',\n",
    "        'GEN_Bu_p_inSSA', \n",
    "        'GEN_Bu_p_LS', \n",
    "        'GEN_Red_per_LSbldg',\n",
    "        'GEN_Or_per_LSblg', \n",
    "        'GEN_Yel_per_LSSAb', \n",
    "        'GEN_RED_per_SSAbldg',\n",
    "        'GEN_OR_per_SSAbldg',\n",
    "        'GEN_Yellow_per_LSbl',\n",
    "        'TOP_mean_slope',\n",
    "        'TOP_mean_elevation_m', \n",
    "        'TOP_ruggedness_stdev', \n",
    "        'TOP_mean_ruggedness',\n",
    "        'TOP_slope_stdev', \n",
    "        'VUL_poverty_perc',\n",
    "        'GEN_with_coast',\n",
    "        'GEN_coast_length', \n",
    "        'VUL_Housing_Units',\n",
    "        'VUL_StrongRoof_StrongWall', \n",
    "        'VUL_StrongRoof_LightWall',\n",
    "        'VUL_StrongRoof_SalvageWall', \n",
    "        'VUL_LightRoof_StrongWall',\n",
    "        'VUL_LightRoof_LightWall', \n",
    "        'VUL_LightRoof_SalvageWall',\n",
    "        'VUL_SalvagedRoof_StrongWall',\n",
    "        'VUL_SalvagedRoof_LightWall',\n",
    "        'VUL_SalvagedRoof_SalvageWall', \n",
    "        'VUL_vulnerable_groups',\n",
    "        'VUL_pantawid_pamilya_beneficiary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Full dataset for feature selection\n",
    "\n",
    "df=combined_input_data.dropna()\n",
    " \n",
    "#combined_input_data = combined_input_data[combined_input_data['DAM_perc_dmg'].notnull()]\n",
    "X = df[features]\n",
    "y = df[\"DAM_perc_dmg\"]\n",
    "\n",
    "# Setting the train and the test sets for obtaining performance estimate\n",
    "df_train_list, df_test_list = splitting_train_test(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Random Forest\n",
    "#Training the optimal model\n",
    "Number of selected features RF Regression: 12\n",
    "\n",
    "Selected features RF Regression:\n",
    "- mean_slope\n",
    "- mean_elevation_m\n",
    "- ruggedness_stdev\n",
    "- mean_ruggedness\n",
    "- area_km2\n",
    "- coast_length\n",
    "- poverty_perc\n",
    "- perimeter\n",
    "- glat\n",
    "- glon\n",
    "- coast_peri_ratio\n",
    "- rainfall_max_6h\n",
    "- rainfall_max_24h\n",
    "- dis_track_min\n",
    "- vmax\n",
    "\n",
    "\n",
    "Selected Parameters RF Regression: \n",
    "- max_depth = 20 #20\n",
    "- min_samples_leaf = 1 #2\n",
    "- min_samples_split = 8 #5\n",
    "- n_estimators = 100 #250/100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "\n",
    "Feature Selection is an important step in devloping a machine learning model.Data features used to train a machine learning model will influence model performance,less important features can have a negative impact on model performance.\n",
    "Feature Selection aims to solve the problem of identifying relevant features from a dataset by removing the less important features, which have little/no contribution to our target variable. Feature selection helps to achieve better model accuracy.\n",
    "\n",
    "There are different techniques for feature selection. For this research we used Recursive feature elimination (RFE),which is a feature selection method that fits a model and removes the weakest feature (or features) until the specified number of features is reached. Features are ranked by the model’s coef_ or feature_importances_ attributes, and by recursively eliminating a small number of features per loop, RFE attempts to eliminate dependencies and collinearity that may exist in the model.\n",
    "To find the optimal number of features we applied cross-validation with RFE on the entire data set. \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Setting input varialbes\n",
    "rf_search_space = [\n",
    "    {\n",
    "        \"estimator__n_estimators\": [100, 150],\n",
    "        \"estimator__max_depth\": [20, None],\n",
    "        \"estimator__min_samples_split\": [4, 5, 8],\n",
    "        \"estimator__min_samples_leaf\":[1, 3, 5],\n",
    "    }\n",
    "]\n",
    "\n",
    "(\n",
    "    selected_features_rf_regr,\n",
    "    selected_params_rf_regr_full,\n",
    ") = rf_regression_features(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    features=features,\n",
    "    search_space=rf_search_space,\n",
    "    min_features_to_select=1,\n",
    "    cv_splits=3,\n",
    "    GS_score=\"neg_root_mean_squared_error\",\n",
    "    GS_randomized=False,\n",
    "    GS_n_iter=10,\n",
    "    verbose=10,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Number of selected features RF Regression {len(selected_features_rf_regr)}\"\n",
    ")\n",
    "print(f\"Selected features RF Regression: {selected_features_rf_regr}\")\n",
    "print(f\"Selected Parameters RF Regression: {selected_params_rf_regr_full}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on output previous cell\n",
    "selected_features_rf_regr=[\n",
    "    'HAZ_rainfall_Total', \n",
    "    'HAZ_rainfall_max_6h',\n",
    "    'HAZ_rainfall_max_24h',\n",
    "    'HAZ_v_max', \n",
    "    'HAZ_dis_track_min',\n",
    "    'GEN_landslide_per', \n",
    "    'GEN_stormsurge_per',\n",
    "    'GEN_Bu_p_inSSA', \n",
    "    'GEN_Bu_p_LS',\n",
    "    'GEN_Red_per_LSbldg',\n",
    "    'GEN_Yel_per_LSSAb',\n",
    "    'GEN_RED_per_SSAbldg', \n",
    "    'GEN_OR_per_SSAbldg',\n",
    "    'TOP_mean_slope', \n",
    "    'TOP_mean_elevation_m', \n",
    "    'TOP_ruggedness_stdev', \n",
    "    'TOP_mean_ruggedness', \n",
    "    'TOP_slope_stdev', \n",
    "    'VUL_poverty_perc', \n",
    "    'GEN_coast_length',\n",
    "    'VUL_Housing_Units', \n",
    "    'VUL_StrongRoof_StrongWall', \n",
    "    'VUL_StrongRoof_LightWall',\n",
    "    'VUL_StrongRoof_SalvageWall',\n",
    "    'VUL_LightRoof_LightWall', \n",
    "    'VUL_LightRoof_SalvageWall', \n",
    "    'VUL_SalvagedRoof_StrongWall',\n",
    "    'VUL_SalvagedRoof_LightWall', \n",
    "    'VUL_SalvagedRoof_SalvageWall',\n",
    "    'VUL_vulnerable_groups', \n",
    "    'VUL_pantawid_pamilya_beneficiary'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Hyper Parameter optimization  \n",
    "\n",
    "Machine learning models have hyperparameters that you must set in order to customize the model to your dataset. Often the general effects of hyperparameters on a model are known, but how to best set a hyperparameter and combinations of interacting hyperparameters for a given dataset is challenging. There are often general rules of thumb for configuring hyperparameters. A better approach is to objectively search different values for model hyperparameters and choose a subset that results in a model that achieves the best performance on a given dataset. This is called hyperparameter optimization or hyperparameter tuning and is available in the scikit-learn Python machine learning library. [Source](https://machinelearningmastery.com/) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Hyperparameters are essentila components for machine learning algorithms, they control behaviour and performance of a machine learning model. For a learning algorithm optimal hyperparameter selection, hyperparameter tuning is esstil first step as it helps to achive best model performance on the data set with a reasonable amount of time.[source](https://www.sciencedirect.com/science/article/pii/S1674862X19300047)\n",
    "\n",
    "To reduce the bias in performance evaluation, model selection should be treated as an integral part of the model fitting procedure, and should be conducted independently in each trial in order to prevent selection bias.[source](https://www.jmlr.org/papers/v11/cawley10a.html)\n",
    "\n",
    "There are different techniques for Hyperparameters, for this research we used neasted K-fold cross validation technique. \n",
    "Nested cross-validation uses inner and outer loops when optimizing the hyperparameters of a model on a dataset, and when comparing and selecting a model for the dataset. This reduced biased evaluation of model performance as different dataset are used to for hyperparameter tunning and model selection.\n",
    "\n",
    "In our implementation of nested CV the outer loop iterates over typhoon events in our datasets, holiding data for one typhoon for test set and assigning the remaining data as training set. In the inner loop a k-fold CV is applied on the training dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Setting input varialbes\n",
    "\n",
    "\n",
    "rf_search_space = [\n",
    "    {\n",
    "        \"rf__n_estimators\": [100],\n",
    "        \"rf__max_depth\": [22],\n",
    "        \"rf__min_samples_split\": [2],\n",
    "        \"rf__min_samples_leaf\": [3],\n",
    "    }\n",
    "]\n",
    "\n",
    "df_predicted_rf_regr, selected_params_rf_regr = rf_regression_performance(\n",
    "    df_train_list=df_train_list,\n",
    "    df_test_list=df_test_list,\n",
    "    y_var='DAM_perc_dmg',\n",
    "    features=selected_features_rf_regr,\n",
    "    search_space=rf_search_space,\n",
    "    cv_splits=5,\n",
    "    GS_score=\"neg_root_mean_squared_error\",\n",
    "    GS_randomized=False,\n",
    "    GS_n_iter=10,\n",
    "    verbose=10,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selected_Parameters={'rf__max_depth': 22, \n",
    "# 'rf__min_samples_leaf': 3,\n",
    "# 'rf__min_samples_split': 2,\n",
    "# 'rf__n_estimators': 100}\n",
    "\n",
    "#Train score: 0.006772373372310738\n",
    "#Test score: 0.005727847779853535\n",
    "\n",
    "\n",
    "path = \"./models/output/02/selected_params_rf_regr.p\"\n",
    "\n",
    "pickle.dump(selected_params_rf_regr, open(path, \"wb\"))\n",
    "\n",
    "file_name = \"./models/output/02/df_predicted_rf_regr.csv\"\n",
    "\n",
    "df_predicted_rf_regr.to_csv(file_name)\n",
    "\n",
    "#df_predicted_rf_regr=pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Regression \n",
    "Obtaining the optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Full dataset for feature selection\n",
    "\n",
    "combined_input_data = combined_input_data[combined_input_data['DAM_perc_dmg'].notnull()]\n",
    "X = combined_input_data[features]\n",
    "y = combined_input_data[\"DAM_perc_dmg\"]\n",
    "\n",
    "# Setting the train and the test sets for obtaining performance estimate\n",
    "df_train_list, df_test_list = splitting_train_test(combined_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_search_space = [\n",
    "    {\n",
    "        \"estimator__learning_rate\": [0.1, 0.5, 1],\n",
    "        \"estimator__gamma\": [0.1, 0.5, 2],\n",
    "        \"estimator__max_depth\": [6, 8],\n",
    "        \"estimator__reg_lambda\": [0.001, 0.1, 1],\n",
    "        \"estimator__n_estimators\": [100, 200],\n",
    "        \"estimator__colsample_bytree\": [0.5, 0.7],\n",
    "    }\n",
    "]\n",
    "\n",
    "selected_features_xgb_regr, selected_params_xgb_regr_full = xgb_regression_features(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    features=features,\n",
    "    search_space=xgb_search_space,\n",
    "    min_features_to_select=1,\n",
    "    cv_splits=5,\n",
    "    GS_score=\"neg_root_mean_squared_error\",\n",
    "    objective=\"reg:squarederror\",\n",
    "    GS_randomized=True,\n",
    "    GS_n_iter=50,\n",
    "    verbose=10,\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Number of selected features XGBoost Regression {len(selected_features_xgb_regr)}\")\n",
    "print(f\"Selected features XGBoost Regression: {selected_features_xgb_regr}\")\n",
    "print(f\"Selected Parameters XGBoost Regression: {selected_params_xgb_regr_full}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining performance estimate¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the selected features for XGB\n",
    "\n",
    "selected_features_xgb_regr = ['HAZ_rainfall_Total',\n",
    " 'HAZ_v_max',\n",
    " 'HAZ_dis_track_min',\n",
    " 'GEN_landslide_per',\n",
    " 'TOP_mean_elevation_m',\n",
    " 'TOP_mean_ruggedness',\n",
    " 'VUL_Housing_Units',\n",
    " 'VUL_StrongRoof_StrongWall',\n",
    " 'VUL_StrongRoof_LightWall',\n",
    " 'VUL_LightRoof_StrongWall',\n",
    " 'VUL_vulnerable_groups',\n",
    " 'VUL_pantawid_pamilya_beneficiary']\n",
    "\n",
    "selected_params_xgb_regr_full={'estimator__reg_lambda': 0.001,\n",
    " 'estimator__n_estimators': 200,\n",
    " 'estimator__max_depth': 6,\n",
    " 'estimator__learning_rate': 0.1,\n",
    " 'estimator__gamma': 0.1,\n",
    " 'estimator__colsample_bytree': 0.5}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parameter optimization first based on selected model features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xgb_search_space = [\n",
    "    {\n",
    "        \"xgb__learning_rate\": [0.1, 0.5, 1],\n",
    "        \"xgb__gamma\": [0.1, 0.5, 2],\n",
    "        \"xgb__max_depth\": [6, 8],\n",
    "        \"xgb__reg_lambda\": [0.001, 0.1, 1],\n",
    "        \"xgb__n_estimators\": [100, 200],\n",
    "        \"xgb__colsample_bytree\": [0.5, 0.7],\n",
    "    }\n",
    "]\n",
    "\n",
    "df_predicted_xgb_regr, selected_params_xgb_regr = xgb_regression_performance(\n",
    "    df_train_list=df_train_list,\n",
    "    df_test_list=df_test_list,\n",
    "    y_var='DAM_perc_dmg',\n",
    "    features=selected_features_xgb_regr,\n",
    "    search_space=xgb_search_space,\n",
    "    cv_splits=5,\n",
    "    objective=\"reg:squarederror\",\n",
    "    GS_score=\"neg_root_mean_squared_error\",\n",
    "    GS_randomized=True,\n",
    "    GS_n_iter=50,\n",
    "    verbose=10,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"./models/output/02/selected_params_xgb_regr.p\"\n",
    "\n",
    "pickle.dump(selected_params_xgb_regr, open(file_name, \"wb\"))\n",
    "\n",
    "file_name = \"./models/output/02/df_predicted_xgb_regr.csv\"\n",
    "\n",
    "df_predicted_xgb_regr.to_csv(file_name)\n",
    "\n",
    " \n",
    "#df_predicted_xgb_regr=pd.read_csv(path)\n",
    "### Benchmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict the average\n",
    "df_predicted_mean = pd.DataFrame(columns=[\"typhoon\", \"actual\", \"predicted\"])\n",
    "\n",
    "for i in range(len(df_train_list)):\n",
    "\n",
    "    train = df_train_list[i]\n",
    "    test = df_test_list[i]\n",
    "\n",
    "    y_train = train[\"DAM_perc_dmg\"]\n",
    "    y_test = test[\"DAM_perc_dmg\"]\n",
    "\n",
    "    y_test_pred = [np.mean(y_train)] * len(y_test)\n",
    "\n",
    "    df_predicted_temp = pd.DataFrame(\n",
    "        {\"typhoon\": test[\"typhoon\"], \"actual\": y_test, \"predicted\": y_test_pred}\n",
    "    )\n",
    "\n",
    "    df_predicted_mean = pd.concat([df_predicted_mean, df_predicted_temp])\n",
    "\n",
    "file_name = \"./models/output/02/df_predicted_mean.csv\"\n",
    "\n",
    "df_predicted_mean.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simle Linear Regression with Wind Speed\n",
    "input_variable = \"HAZ_v_max\"\n",
    "df_predicted_lr = pd.DataFrame(columns=[\"typhoon\", \"actual\", \"predicted\"])\n",
    "\n",
    "for i in range(len(df_train_list)):\n",
    "\n",
    "    train = df_train_list[i]\n",
    "    test = df_test_list[i]\n",
    "\n",
    "    x_train = train[input_variable].values.reshape(-1, 1)\n",
    "    y_train = train[\"DAM_perc_dmg\"].values.reshape(-1, 1)\n",
    "\n",
    "    x_test = test[input_variable].values.reshape(-1, 1)\n",
    "    y_test = test[\"DAM_perc_dmg\"]\n",
    "\n",
    "    model = LinearRegression()\n",
    "    lr_fitted = model.fit(x_train, y_train)\n",
    "\n",
    "    y_pred_train = lr_fitted.predict(x_train)\n",
    "    y_pred_test = lr_fitted.predict(x_test)\n",
    "    y_pred_test = y_pred_test.tolist()\n",
    "    y_pred_test = [val for sublist in y_pred_test for val in sublist]\n",
    "\n",
    "    df_predicted_temp = pd.DataFrame(\n",
    "        {\"typhoon\": test[\"typhoon\"], \"actual\": y_test, \"predicted\": y_pred_test}\n",
    "    )\n",
    "\n",
    "    df_predicted_lr = pd.concat([df_predicted_lr, df_predicted_temp])\n",
    "    \n",
    "file_name = \"./models/output/02/df_predicted_lr.csv\"\n",
    "\n",
    "df_predicted_lr.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Results \n",
    "\n",
    "models = {\n",
    "    \"Random Forest\": df_predicted_rf_regr,\n",
    "    \"XGBoost\": df_predicted_xgb_regr,\n",
    "    \"Average\": df_predicted_mean,\n",
    "    \"Simple Linear Regression\": df_predicted_lr,\n",
    "}\n",
    "\n",
    "mae = []\n",
    "rmse = []\n",
    "\n",
    "# add 'list' if error\n",
    "for df_temp in models.values():\n",
    "    mae.append(mean_absolute_error(df_temp[\"actual\"], df_temp[\"predicted\"]))\n",
    "    rmse.append(mean_squared_error(df_temp[\"actual\"], df_temp[\"predicted\"], squared=False))\n",
    "\n",
    "df_results_regr = pd.DataFrame({\"Models\": list(models.keys()), \"MAE\": mae, \"RMSE\": rmse})\n",
    "display(df_results_regr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification\n",
    "This section obtain the optimal Binary Classification models and the performance estimates, \n",
    "for a 10% threshold. Two models are implemented: Random Forest Classifier, XGBoost Classifier. \n",
    "First, the model is trained on the full dataset to obtain the optimal features followed by a model \n",
    "that obtains the performance estimate using Nested Cross Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df=combined_input_data.dropna()\n",
    " \n",
    "#combined_input_data = combined_input_data[combined_input_data['DAM_perc_dmg'].notnull()]\n",
    "#X = df[features]\n",
    "#y = df[\"DAM_binary_dmg\"]\n",
    "\n",
    "# Setting the train and the test sets for obtaining performance estimate\n",
    "df_train_list, df_test_list = splitting_train_test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_rf_binary=selected_features_rf_regr\n",
    "rf_search_space = [\n",
    "    {\n",
    "        \"rf__n_estimators\": [100, 250],\n",
    "        \"rf__max_depth\": [20, None],\n",
    "        \"rf__min_samples_split\": [2, 8, 15],\n",
    "        \"rf__min_samples_leaf\": [1, 3, 5],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Obtaining the performance estimate\n",
    "df_predicted_rf_binary, selected_params_rf_binary = rf_binary_performance(\n",
    "    df_train_list=df_train_list,\n",
    "    df_test_list=df_test_list,\n",
    "    y_var='DAM_binary_dmg',\n",
    "    features=selected_features_rf_binary,\n",
    "    search_space=rf_search_space,\n",
    "    stratK=True,\n",
    "    cv_splits=5,\n",
    "    class_weight=\"balanced\",\n",
    "    GS_score=\"f1\",\n",
    "    GS_randomized=False,\n",
    "    GS_n_iter=10,\n",
    "    verbose=10,\n",
    ")\n",
    "\n",
    "file_name = \"./models/output/02/selected_params_rf_binary.p\"\n",
    "\n",
    "pickle.dump(selected_params_rf_binary, open(file_name, \"wb\"))\n",
    "\n",
    "file_name = \"./models/output/02/df_predicted_rf_binary.csv\"\n",
    "\n",
    "df_predicted_rf_binary.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### xgb classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_list, df_test_list = splitting_train_test(combined_input_data)\n",
    "\n",
    "selected_features_xgb_binary=selected_features_xgb_regr\n",
    "\n",
    "# Setting the XGBoost search grid\n",
    "xgb_search_space = [\n",
    "    {\n",
    "        \"xgb__learning_rate\": [0.1, 0.5, 1],\n",
    "        \"xgb__gamma\": [0.1, 0.5, 2],\n",
    "        \"xgb__max_depth\": [6, 8],\n",
    "        \"xgb__reg_lambda\": [0.001, 0.1, 1],\n",
    "        \"xgb__n_estimators\": [100, 200],\n",
    "        \"xgb__colsample_bytree\": [0.5, 0.7],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Obtaining the performance estimate\n",
    "df_predicted_xgb_binary, selected_params_xgb_binary = xgb_binary_performance(\n",
    "    df_train_list=df_train_list,\n",
    "    df_test_list=df_test_list,\n",
    "    y_var='DAM_binary_dmg',\n",
    "    features=selected_features_xgb_binary,\n",
    "    search_space=xgb_search_space,\n",
    "    stratK=True,\n",
    "    cv_splits=5,\n",
    "    objective=\"binary:hinge\",\n",
    "    GS_score=\"f1\",\n",
    "    GS_randomized=True,\n",
    "    GS_n_iter=50,\n",
    "    verbose=10,\n",
    ")\n",
    "\n",
    "file_name = \"./models/output/02/selected_params_xgb_binary.p\"\n",
    "\n",
    "pickle.dump(selected_params_xgb_binary, open(file_name, \"wb\"))\n",
    "\n",
    "file_name = \"./models/output/02/df_predicted_xgb_binary.csv\"\n",
    "\n",
    "df_predicted_xgb_binary.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_predicted_random = pd.DataFrame(columns=[\"typhoon\", \"actual\", \"predicted\"])\n",
    "\n",
    "for i in range(len(df_train_list)):\n",
    "\n",
    "    train = df_train_list[i]\n",
    "    test = df_test_list[i]\n",
    "\n",
    "    y_train = train[\"DAM_binary_dmg\"]\n",
    "    y_test = test[\"DAM_binary_dmg\"]\n",
    "\n",
    "    y_pred_test = unweighted_random(y_train, y_test)\n",
    "    df_predicted_temp = pd.DataFrame(\n",
    "        {\"year\": test[\"year\"], \"actual\": y_test, \"predicted\": y_pred_test}\n",
    "    )\n",
    "\n",
    "    df_predicted_random = pd.concat([df_predicted_random, df_predicted_temp])\n",
    "\n",
    "\n",
    "df_predicted_random_weighted = pd.DataFrame(columns=[\"typhoon\", \"actual\", \"predicted\"])\n",
    "\n",
    "for i in range(len(df_train_list)):\n",
    "\n",
    "    train = df_train_list[i]\n",
    "    test = df_test_list[i]\n",
    "\n",
    "    y_train = train[\"DAM_binary_dmg\"]\n",
    "    y_test = test[\"DAM_binary_dmg\"]\n",
    "\n",
    "    y_pred_test = weighted_random(y_train, y_test)\n",
    "    df_predicted_temp = pd.DataFrame(\n",
    "        {\"typhoon\": test[\"typhoon\"], \"actual\": y_test, \"predicted\": y_pred_test}\n",
    "    )\n",
    "\n",
    "    df_predicted_random_weighted = pd.concat(\n",
    "        [df_predicted_random_weighted, df_predicted_temp]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = {\n",
    "    \"Random Fores\": df_predicted_rf_binary,\n",
    "    \"XGBoost\": df_predicted_xgb_binary,\n",
    "    \"Random\": df_predicted_random,\n",
    "    \"Weighted Random\": df_predicted_random_weighted,\n",
    "}\n",
    "\n",
    "f1 = []\n",
    "precision = []\n",
    "recall = []\n",
    "\n",
    "# add 'list' if error\n",
    "for df_temp in models.values():\n",
    "    f1.append(f1_score(list(df_temp[\"actual\"]), list(df_temp[\"predicted\"])))\n",
    "    precision.append(precision_score(list(df_temp[\"actual\"]), list(df_temp[\"predicted\"])))\n",
    "    recall.append(recall_score(list(df_temp[\"actual\"]), list(df_temp[\"predicted\"])))\n",
    "\n",
    "df_results_binary = pd.DataFrame(\n",
    "    {\"Models\": list(models.keys()), \"F1 score\": f1, \"Recall\": recall, \"Precision\": precision}\n",
    ")\n",
    "#%%\n",
    "display(df_results_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climada_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
