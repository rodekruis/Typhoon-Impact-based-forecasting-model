{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7717cab",
   "metadata": {},
   "source": [
    "### IBTrACS data\n",
    "For historical typhoon events we used IBTrACS data, which comes from the IBTracs project\n",
    "\n",
    "The IBTrACS project:\n",
    "    Contains the most complete global set of historical tropical cyclones available\n",
    "    Combines information from numerous tropical cyclone datasets\n",
    "    Simplifies inter-agency comparisons by providing storm data from multiple sources in one place\n",
    "    Combines recent and historical storm data in one dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf21c8c9",
   "metadata": {},
   "source": [
    "### Parametric wind field modelling \n",
    "\n",
    "A parametric representation of the pressure and wind field in tropical cyclones facilitates abstract representation of these meteorological systems using just a few physically meaningful parameters [Depperman, 1947; Holland, 1980; Jelesnianski, 1965; Schloemer, 1954]. These include the central pressure deficit, the maximum wind speed, and the radius to the maximum wind speed.\n",
    "for our Project we used parametric wind field model based on the Holands 2008 method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b5494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% import libraries from local directory\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from os.path import relpath\n",
    "from pybufrkit.decoder import Decoder\n",
    "from geopandas.tools import sjoin\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.ops import nearest_points\n",
    "\n",
    "import logging\n",
    "\n",
    "decoder = Decoder()\n",
    "\n",
    "from pathlib import Path\n",
    "# Get the current working directory\n",
    "current_dir = Path.cwd()\n",
    "\n",
    "wor_dir = current_dir / Path('../../')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3853318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path = current_dir / Path('../../src')\n",
    "os.chdir(relative_path)\n",
    "# Importing local libraries       \n",
    "from climada.hazard import Centroids, TropCyclone, TCTracks\n",
    "from climada.hazard.tc_tracks import estimate_roci, estimate_rmw\n",
    "from climada.hazard.tc_tracks_forecast import TCForecast\n",
    "from typhoonmodel.utility_fun import track_data_clean, Check_for_active_typhoon, Sendemail, Hwrf_data, plot_intensity, initialize\n",
    "from typhoonmodel.utility_fun import Rainfall_data\n",
    "from typhoonmodel.utility_fun.forecast_process import Forecast\n",
    "from climada.util import coordinates  \n",
    "from typhoonmodel.utility_fun.settings import *\n",
    "from typhoonmodel.utility_fun.dynamicDataDb import DatabaseManager\n",
    "\n",
    "os.chdir(wor_dir)\n",
    "\n",
    "initialize.setup_logger()\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce702a0e",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08541c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_tracks(forcast_df):\n",
    "        track = xr.Dataset(\n",
    "            data_vars={\n",
    "                \"max_sustained_wind\": (\n",
    "                    \"time\",\n",
    "                    0.514444\n",
    "                    * forcast_df.max_sustained_wind.values,  # conversion from kn to meter/s\n",
    "                ),\n",
    "                \"environmental_pressure\": (\n",
    "                    \"time\",\n",
    "                    forcast_df.environmental_pressure.values,\n",
    "                ),\n",
    "                \"central_pressure\": (\"time\", forcast_df.central_pressure.values),\n",
    "                \"lat\": (\"time\", forcast_df.lat.values),\n",
    "                \"lon\": (\"time\", forcast_df.lon.values),\n",
    "                \"radius_max_wind\": (\"time\", forcast_df.radius_max_wind.values),\n",
    "                \"radius_oci\": (\"time\", forcast_df.radius_oci.values),\n",
    "                \"time_step\": (\n",
    "                    \"time\",\n",
    "                    np.full_like(forcast_df.time_step.values, 3, dtype=float),\n",
    "                ),\n",
    "            },\n",
    "            coords={\"time\": forcast_df.time.values,},\n",
    "            attrs={\n",
    "                \"max_sustained_wind_unit\": \"m/s\",\n",
    "                \"central_pressure_unit\": \"mb\",\n",
    "                \"name\": forcast_df.name,\n",
    "                \"sid\": forcast_df.sid,  # +str(forcast_df.ensemble_number),\n",
    "                \"orig_event_flag\": forcast_df.orig_event_flag,\n",
    "                \"data_provider\": forcast_df.data_provider,\n",
    "                \"id_no\": forcast_df.id_no,\n",
    "                \"basin\": forcast_df.basin,\n",
    "                \"category\": forcast_df.category,\n",
    "            },\n",
    "        )\n",
    "        track = track.set_coords([\"lat\", \"lon\"])\n",
    "        return track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9194353f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%######################################################\n",
    "####### VARIABLES TO SET\n",
    "#########################################################\n",
    "year_range = (2006, 2022)\n",
    "\n",
    "# Laoding PH admin file and setting centroids\n",
    "# Instead of one grid point for each municipalities: uses a range of gridpoints\n",
    "# calculates value for each point\n",
    "# Eventually select a specific point in municipality based on condition\n",
    "# maximum windspeed and minimum track distance\n",
    "file_name = \"data-raw/gis_data/phl_admin3_simpl2.geojson\"\n",
    "\n",
    "#path = os.path.join(wor_dir, file_name)\n",
    "\n",
    "admin = gpd.read_file(file_name)\n",
    "\n",
    "minx, miny, maxx, maxy = admin.total_bounds\n",
    "print(minx, miny, maxx, maxy)\n",
    "\n",
    "cent = Centroids()\n",
    "cent.set_raster_from_pnt_bounds((minx, miny, maxx, maxy), res=0.05)\n",
    "cent.check()\n",
    "cent.plot()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feed443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('./data/wind_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b99cc43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loads an excel sheet with Local_name, International_Name and year\n",
    "# Setting directory for typhoon metadata csv file\n",
    "typhoon_metadata_filename =\"data/metadata_typhoons.csv\"\n",
    "typhoon_metadata = pd.read_csv(typhoon_metadata_filename, delimiter=\",\")\n",
    "typhoons = list(typhoon_metadata.typhoon.values)\n",
    "# if there is already wind data in the project folder  wind_data/output\n",
    "typhoons_with_wind_data = [items.split('_')[0] for items in os.listdir('./data/wind_data/output/')]\n",
    "\n",
    "typhoons_withoutwind_data = [items for items in typhoons if items not in typhoons_with_wind_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0e3354",
   "metadata": {},
   "outputs": [],
   "source": [
    "typhoon_events = typhoons_withoutwind_data#list(typhoon_metadata.typhoon.values)\n",
    "typhoon_events=[event.upper() for event in typhoon_events]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc5428d",
   "metadata": {},
   "outputs": [],
   "source": [
    "typoon_event=typhoon_events\n",
    "sel_ibtracs = TCTracks()\n",
    "# Set year range for which data should be collected\n",
    "sel_ibtracs.read_ibtracs_netcdf(\n",
    "    provider=\"usa\", year_range=year_range, basin=\"WP\", correct_pres=False\n",
    ")\n",
    "\n",
    "Typhoons = TCTracks()\n",
    "# Select typhoons that are in the typhoon event sheet\n",
    "Typhoons.data = [\n",
    "    tr for tr in sel_ibtracs.data if (tr.name + tr.sid[:4]) in typoon_event\n",
    "]\n",
    "\n",
    "# Plot the typhoon track\n",
    "ax = Typhoons.plot()\n",
    "ax.get_legend()._loc = 1\n",
    "ax.set_title(typoon_event, fontsize=5)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9785e697",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%######################################################\n",
    "####### START PROCESSING\n",
    "#########################################################\n",
    " \n",
    "for typhoon_name in typhoon_events:\n",
    "\n",
    "    typoon_event = [typhoon_name]\n",
    "    print(typoon_event)\n",
    "\n",
    "    sel_ibtracs = TCTracks()\n",
    "    # Set year range for which data should be collected\n",
    "    sel_ibtracs.read_ibtracs_netcdf(\n",
    "        provider=\"usa\", year_range=year_range, basin=\"WP\", correct_pres=False\n",
    "    )\n",
    "\n",
    "    Typhoons = TCTracks()\n",
    "    # Select typhoons that are in the typhoon event sheet\n",
    "    Typhoons.data = [\n",
    "        tr for tr in sel_ibtracs.data if (tr.name+tr.sid) in typoon_event\n",
    "    ]\n",
    "\n",
    "    # Plot the typhoon track\n",
    "    #ax = Typhoons.plot()\n",
    "    #ax.get_legend()._loc = 1\n",
    "    #ax.set_title(typoon_event, fontsize=14)\n",
    "    #plt.show()\n",
    "    #plt.close()\n",
    "\n",
    "    # Select names and storm id's of storms\n",
    "    #names = [[tr.name, tr.sid] for tr in Typhoons.data]\n",
    "\n",
    "    df = pd.DataFrame(data=cent.coord)\n",
    "    df[\"centroid_id\"] = \"id\" + (df.index).astype(str)\n",
    "    centroid_idx = df[\"centroid_id\"].values\n",
    "    ncents = cent.size\n",
    "    df = df.rename(columns={0: \"lat\", 1: \"lon\"})   \n",
    "\n",
    "\n",
    "    tracks = TCTracks()\n",
    "    tracks.data = [adjust_tracks(tr) for tr in Typhoons.data]\n",
    "    tracks.equal_timestep(0.5)\n",
    "\n",
    "    if tracks.data !=[]:\n",
    "        # define a new typhoon class\n",
    "        TYphoon = TropCyclone()\n",
    "        TYphoon.set_from_tracks(tracks, cent, store_windfields=True)\n",
    "\n",
    "        # plot intensity\n",
    "        #TYphoon.plot_intensity(event=Typhoons.data[0].sid)\n",
    "        #plt.show()\n",
    "        #plt.close()\n",
    "\n",
    "        df = pd.DataFrame(data=cent.coord)\n",
    "        df[\"centroid_id\"] = \"id\" + (df.index).astype(str)\n",
    "        centroid_idx = df[\"centroid_id\"].values\n",
    "        ncents = cent.size\n",
    "        df = df.rename(columns={0: \"lat\", 1: \"lon\"})\n",
    "        threshold = 0.1\n",
    "\n",
    "        list_intensity = []\n",
    "        distan_track = []\n",
    "\n",
    "        for tr in tracks.data:\n",
    "            print(tr.name)\n",
    "\n",
    "            track = TCTracks()\n",
    "            typhoon = TropCyclone()\n",
    "            track.data = [tr]\n",
    "            typhoon.set_from_tracks(track, cent, store_windfields=True)\n",
    "            windfield = typhoon.windfields\n",
    "            nsteps = windfield[0].shape[0]\n",
    "            centroid_id = np.tile(centroid_idx, nsteps)\n",
    "            intensity_3d = windfield[0].toarray().reshape(nsteps, ncents, 2)\n",
    "            intensity = np.linalg.norm(intensity_3d, axis=-1).ravel()\n",
    "\n",
    "            timesteps = np.repeat(tr.time.values, ncents)\n",
    "            timesteps = timesteps.reshape((nsteps, ncents)).ravel()\n",
    "            inten_tr = pd.DataFrame(\n",
    "                {\"centroid_id\": centroid_id, \"value\": intensity, \"timestamp\": timesteps,}\n",
    "            )\n",
    "\n",
    "            inten_tr = inten_tr[inten_tr.value > threshold]\n",
    "\n",
    "            inten_tr[\"storm_id\"] = tr.sid\n",
    "            list_intensity.append(inten_tr)\n",
    "            distan_track1 = []\n",
    "            for index, row in df.iterrows():\n",
    "                dist = np.min(\n",
    "                    np.sqrt(\n",
    "                        np.square(tr.lat.values - row[\"lat\"])\n",
    "                        + np.square(tr.lon.values - row[\"lon\"])\n",
    "                    )\n",
    "                )\n",
    "                distan_track1.append(dist * 111)\n",
    "            dist_tr = pd.DataFrame({\"centroid_id\": centroid_idx, \"value\": distan_track1})\n",
    "            dist_tr[\"storm_id\"] = tr.sid\n",
    "            distan_track.append(dist_tr)\n",
    "\n",
    "        df_ = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.lon, df.lat))\n",
    "        df_.crs = {\"init\": \"epsg:4326\"}\n",
    "        df_ = df_.to_crs(\"EPSG:4326\")\n",
    "        df_admin = sjoin(df_, admin, how=\"left\")\n",
    "        # To remove points that are in water\n",
    "        df_admin = df_admin.dropna()\n",
    "\n",
    "        # For some municipalities, there is missing data\n",
    "        # For these, fill with the value of the nearest centroid with data observed\n",
    "        mun_missing = admin[admin[\"adm3_pcode\"].isin(df_admin[\"adm3_pcode\"]) == False]\n",
    "        print(f\"There are {len(mun_missing)} missing municipalities\")\n",
    "\n",
    "        mun_missing_points = gpd.GeoDataFrame(\n",
    "            mun_missing, geometry=gpd.points_from_xy(mun_missing.glon, mun_missing.glat)\n",
    "        )\n",
    "        pts3 = df_admin.geometry.unary_union\n",
    "\n",
    "        def near(point, pts=pts3):\n",
    "            # find the nearest point and return the corresponding Place value\n",
    "            nearest = df_admin.geometry == nearest_points(point, pts)[1]\n",
    "            # display(df_admin[nearest].centroid_id.values[0])\n",
    "            return df_admin[nearest].centroid_id.values[0]\n",
    "\n",
    "        mun_missing_points[\"centroid_id\"] = mun_missing_points.apply(\n",
    "            lambda row: near(row.geometry), axis=1\n",
    "        )\n",
    "\n",
    "        def match_values_lat(x):\n",
    "\n",
    "            return df_admin[\"lat\"][df_admin[\"centroid_id\"] == x].values[0]\n",
    "\n",
    "        def match_values_lon(x):\n",
    "\n",
    "            return df_admin[\"lon\"][df_admin[\"centroid_id\"] == x].values[0]\n",
    "\n",
    "        def match_values_geo(x):\n",
    "\n",
    "            return df_admin[\"geometry\"][df_admin[\"centroid_id\"] == x].values[0]\n",
    "\n",
    "        mun_missing_points[\"lat\"] = mun_missing_points[\"centroid_id\"].apply(\n",
    "            match_values_lat\n",
    "        )\n",
    "        mun_missing_points[\"lon\"] = mun_missing_points[\"centroid_id\"].apply(\n",
    "            match_values_lon\n",
    "        )\n",
    "        mun_missing_points[\"geometry\"] = mun_missing_points[\"centroid_id\"].apply(\n",
    "            match_values_geo\n",
    "        )\n",
    "\n",
    "        df_admin = df_admin[\n",
    "            [\n",
    "                \"adm3_en\",\n",
    "                \"adm3_pcode\",\n",
    "                \"adm2_pcode\",\n",
    "                \"adm1_pcode\",\n",
    "                \"glat\",\n",
    "                \"glon\",\n",
    "                \"lat\",\n",
    "                \"lon\",\n",
    "                \"centroid_id\",\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        mun_missing_points = mun_missing_points[\n",
    "            [\n",
    "                \"adm3_en\",\n",
    "                \"adm3_pcode\",\n",
    "                \"adm2_pcode\",\n",
    "                \"adm1_pcode\",\n",
    "                \"glat\",\n",
    "                \"glon\",\n",
    "                \"lat\",\n",
    "                \"lon\",\n",
    "                \"centroid_id\",\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        df_admin = pd.concat([df_admin, mun_missing_points])\n",
    "        mun_missing = admin[admin[\"adm3_pcode\"].isin(df_admin[\"adm3_pcode\"]) == False]\n",
    "        print(f\"There are {len(mun_missing)} missing municipalities\")\n",
    "\n",
    "        df_intensity = pd.concat(list_intensity)\n",
    "        df_intensity = pd.merge(df_intensity, df_admin, how=\"outer\", on=\"centroid_id\")\n",
    "\n",
    "        df_intensity = pd.concat(list_intensity)\n",
    "        df_intensity = pd.merge(df_intensity, df_admin, how=\"outer\", on=\"centroid_id\")\n",
    "\n",
    "        df_intensity = df_intensity.dropna()\n",
    "\n",
    "        # Obtains the maximum intensity for each municipality and storm_id combination & also return the count (how often the combination occurs in the set)\n",
    "        df_intensity = (\n",
    "            df_intensity[df_intensity[\"value\"].gt(0)]\n",
    "            .groupby([\"adm3_pcode\", \"storm_id\"], as_index=False)\n",
    "            .agg({\"value\": [\"count\", \"max\"]})\n",
    "        )\n",
    "        # rename columns\n",
    "        df_intensity.columns = [\n",
    "            x for x in [\"adm3_pcode\", \"storm_id\", \"value_count\", \"v_max\"]\n",
    "        ]\n",
    "\n",
    "        df_track = pd.concat(distan_track)\n",
    "        df_track = pd.merge(df_track, df_admin, how=\"outer\", on=\"centroid_id\")\n",
    "        df_track = df_track.dropna()\n",
    "\n",
    "        # Obtains the minimum track distance for each municipality and storm_id combination\n",
    "        df_track_ = df_track.groupby([\"adm3_pcode\", \"storm_id\"], as_index=False).agg(\n",
    "            {\"value\": \"min\"}\n",
    "        )\n",
    "        df_track_.columns = [x for x in [\"adm3_pcode\", \"storm_id\", \"dis_track_min\"]]\n",
    "        typhhon_df = pd.merge(\n",
    "            df_intensity, df_track_, how=\"left\", on=[\"adm3_pcode\", \"storm_id\"]\n",
    "        )\n",
    "\n",
    "        # Check if there are duplicates for municipality and storm_id\n",
    "        duplicate = typhhon_df[\n",
    "            typhhon_df.duplicated(subset=[\"adm3_pcode\", \"storm_id\"], keep=False)\n",
    "        ]\n",
    "        if len(duplicate) != 0:\n",
    "            print(\"There are duplicates, please check\")\n",
    "\n",
    "        fname= typhoon_name.lower()\n",
    "\n",
    "        file_name = (f\"./data/wind_data/output/{fname}_windgrid_output.csv\")\n",
    "        #path = os.path.join(cdir, file_name)\n",
    "        typhhon_df.to_csv(file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bd4784",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climada_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
