{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b6a1514",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "This note book explains the different steps in the machine learning model for the binary classfication model. First the model is trained on the full dataset to obtain the optimal features followed by hyper parameter tunning and model performance estimate using Nested Cross Validation.\n",
    "\n",
    "* Nested Cross Validation for\n",
    "    * Feature selection \n",
    "    * hyper parameter tunning \n",
    "* Performance metrics\n",
    "* Baseline Models\n",
    "\n",
    "### Binary Classification\n",
    "At the end of this section we will obtain  the optimal Binary Classification models and the performance estimates, \n",
    "for a 10% threshold. Two models are implemented: Random Forest Classifier, XGBoost Classifier. \n",
    "First, the model is trained on the full dataset to obtain the optimal features followed by a model \n",
    "that obtains the performance estimate using Nested Cross Validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bcd382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from xgboost import XGBClassifier\n",
    "import os\n",
    "from sklearn.feature_selection import RFECV\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    StratifiedKFold,\n",
    "    KFold,\n",
    ")\n",
    "from sklearn.metrics import f1_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "from numpy.lib.function_base import average\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import (\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    confusion_matrix,\n",
    "    make_scorer,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    StratifiedKFold,\n",
    "    KFold,\n",
    ")\n",
    "from sklearn.feature_selection import SelectKBest, SequentialFeatureSelector\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "import importlib\n",
    "import os\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest,\n",
    "    RFE,\n",
    "    mutual_info_regression,\n",
    "    f_regression,\n",
    "    mutual_info_classif,\n",
    ")\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn.inspection import permutation_importance\n",
    "import xgboost as xgb\n",
    "import random\n",
    "import pickle\n",
    "import openpyxl\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import RFECV\n",
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import geopandas as gpd\n",
    "import random\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac563676",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def binary_damage_class(x):\n",
    "    damage = x[0]   \n",
    "    if damage > 0.1:\n",
    "        value = 1\n",
    "    else:\n",
    "        value = 0\n",
    "    return value\n",
    "\n",
    "\n",
    "def splitting_train_test(df):\n",
    "\n",
    "    # To save the train and test sets\n",
    "    df_train_list = []\n",
    "    df_test_list = []\n",
    "\n",
    "    # List of typhoons that are to be used as a test set \n",
    " \n",
    "    typhoons_with_impact_data=list(np.unique(df.typhoon))\n",
    "\n",
    "    for typhoon in typhoons_with_impact_data:\n",
    "        if len(df[df[\"typhoon\"] == typhoon]) >100:\n",
    "            df_train_list.append(df[df[\"typhoon\"] != typhoon])\n",
    "            df_test_list.append(df[df[\"typhoon\"] == typhoon])\n",
    "\n",
    "    return df_train_list, df_test_list\n",
    "\n",
    "def unweighted_random(y_train, y_test):\n",
    "    options = y_train.value_counts(normalize=True)\n",
    "    y_pred = random.choices(population=list(options.index), k=len(y_test))\n",
    "    return y_pred\n",
    "\n",
    "def weighted_random(y_train, y_test):\n",
    "    options = y_train.value_counts()\n",
    "    y_pred = random.choices(\n",
    "        population=list(options.index), weights=list(options.values), k=len(y_test)\n",
    "    )\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edfd2288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting directory\n",
    "\n",
    "wor_dir=\"C:\\\\Users\\\\ATeklesadik\\\\OneDrive - Rode Kruis\\\\Documents\\\\documents\\\\Typhoon-Impact-based-forecasting-model\\\\IBF-typhoon-model\"\n",
    "\n",
    "os.chdir(wor_dir)\n",
    "\n",
    "cdir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09c601cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions\n",
    "from models.binary_classification.rf_binary import (rf_binary_features,rf_binary_performance,)\n",
    "from models.binary_classification.xgb_binary import (xgb_binary_features,xgb_binary_performance,)\n",
    "from models.regression.rf_regression import (rf_regression_features,rf_regression_performance,)\n",
    "from models.regression.xgb_regression import (xgb_regression_features,xgb_regression_performance,)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfe30206",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['perc_dmg'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ATEKLE~1\\AppData\\Local\\Temp/ipykernel_24164/1936669293.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcombined_input_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data\\\\model_input\\\\combined_input_data.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcombined_input_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"binary_dmg\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcombined_input_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"perc_dmg\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbinary_damage_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"columns\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m combined_input_data =combined_input_data.filter(['typhoon','HAZ_rainfall_Total', \n",
      "\u001b[1;32m~\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3459\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3460\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3461\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3463\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1312\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1314\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m         if needs_i8_conversion(ax.dtype) or isinstance(\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis)\u001b[0m\n\u001b[0;32m   1372\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0muse_interval_msg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1373\u001b[0m                     \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1374\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1376\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['perc_dmg'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "combined_input_data=pd.read_csv(\"data\\\\model_input\\\\combined_input_data.csv\")\n",
    "combined_input_data[\"binary_dmg\"] = combined_input_data[[\"perc_dmg\"]].apply(binary_damage_class, axis=\"columns\")\n",
    "\n",
    "\n",
    "combined_input_data =combined_input_data.filter(['typhoon','HAZ_rainfall_Total', \n",
    "        'HAZ_rainfall_max_6h',\n",
    "        'HAZ_rainfall_max_24h',\n",
    "        'HAZ_v_max',\n",
    "        'HAZ_dis_track_min',\n",
    "        'GEN_landslide_per',\n",
    "        'GEN_stormsurge_per',\n",
    "        'GEN_Bu_p_inSSA', \n",
    "        'GEN_Bu_p_LS', \n",
    "        'GEN_Red_per_LSbldg',\n",
    "        'GEN_Or_per_LSblg', \n",
    "        'GEN_Yel_per_LSSAb', \n",
    "        'GEN_RED_per_SSAbldg',\n",
    "        'GEN_OR_per_SSAbldg',\n",
    "        'GEN_Yellow_per_LSbl',\n",
    "        'TOP_mean_slope',\n",
    "        'TOP_mean_elevation_m', \n",
    "        'TOP_ruggedness_stdev', \n",
    "        'TOP_mean_ruggedness',\n",
    "        'TOP_slope_stdev', \n",
    "        'VUL_poverty_perc',\n",
    "        'GEN_with_coast',\n",
    "        'GEN_coast_length', \n",
    "        'VUL_Housing_Units',\n",
    "        'VUL_StrongRoof_StrongWall', \n",
    "        'VUL_StrongRoof_LightWall',\n",
    "        'VUL_StrongRoof_SalvageWall', \n",
    "        'VUL_LightRoof_StrongWall',\n",
    "        'VUL_LightRoof_LightWall', \n",
    "        'VUL_LightRoof_SalvageWall',\n",
    "        'VUL_SalvagedRoof_StrongWall',\n",
    "        'VUL_SalvagedRoof_LightWall',\n",
    "        'VUL_SalvagedRoof_SalvageWall', \n",
    "        'VUL_vulnerable_groups',\n",
    "        'VUL_pantawid_pamilya_beneficiary',\n",
    "        'DAM_binary_dmg'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d29b0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "features =['HAZ_rainfall_Total', \n",
    "        'HAZ_rainfall_max_6h',\n",
    "        'HAZ_rainfall_max_24h',\n",
    "        'HAZ_v_max',\n",
    "        'HAZ_dis_track_min',\n",
    "        'GEN_landslide_per',\n",
    "        'GEN_stormsurge_per',\n",
    "        'GEN_Bu_p_inSSA', \n",
    "        'GEN_Bu_p_LS', \n",
    "        'GEN_Red_per_LSbldg',\n",
    "        'GEN_Or_per_LSblg', \n",
    "        'GEN_Yel_per_LSSAb', \n",
    "        'GEN_RED_per_SSAbldg',\n",
    "        'GEN_OR_per_SSAbldg',\n",
    "        'GEN_Yellow_per_LSbl',\n",
    "        'TOP_mean_slope',\n",
    "        'TOP_mean_elevation_m', \n",
    "        'TOP_ruggedness_stdev', \n",
    "        'TOP_mean_ruggedness',\n",
    "        'TOP_slope_stdev', \n",
    "        'VUL_poverty_perc',\n",
    "        'GEN_with_coast',\n",
    "        'GEN_coast_length', \n",
    "        'VUL_Housing_Units',\n",
    "        'VUL_StrongRoof_StrongWall', \n",
    "        'VUL_StrongRoof_LightWall',\n",
    "        'VUL_StrongRoof_SalvageWall', \n",
    "        'VUL_LightRoof_StrongWall',\n",
    "        'VUL_LightRoof_LightWall', \n",
    "        'VUL_LightRoof_SalvageWall',\n",
    "        'VUL_SalvagedRoof_StrongWall',\n",
    "        'VUL_SalvagedRoof_LightWall',\n",
    "        'VUL_SalvagedRoof_SalvageWall', \n",
    "        'VUL_vulnerable_groups',\n",
    "        'VUL_pantawid_pamilya_beneficiary']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e1f45",
   "metadata": {},
   "source": [
    "####  Random forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a0a21a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>typhoon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>durian2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>durian2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>durian2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>durian2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>durian2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13246</th>\n",
       "      <td>noul2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13247</th>\n",
       "      <td>noul2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13248</th>\n",
       "      <td>noul2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13249</th>\n",
       "      <td>noul2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13250</th>\n",
       "      <td>noul2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13251 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          typhoon\n",
       "0      durian2006\n",
       "1      durian2006\n",
       "2      durian2006\n",
       "3      durian2006\n",
       "4      durian2006\n",
       "...           ...\n",
       "13246    noul2015\n",
       "13247    noul2015\n",
       "13248    noul2015\n",
       "13249    noul2015\n",
       "13250    noul2015\n",
       "\n",
       "[13251 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a4e712",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=combined_input_data.dropna()\n",
    " \n",
    "#combined_input_data = combined_input_data[combined_input_data['DAM_perc_dmg'].notnull()]\n",
    "X = df[features]\n",
    "y = df[\"DAM_binary_dmg\"]\n",
    "\n",
    "# Setting the train and the test sets for obtaining performance estimate\n",
    "df_train_list, df_test_list = splitting_train_test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65341361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the random forest search grid\n",
    "rf_search_space = [\n",
    "    {\n",
    "        \"estimator__n_estimators\": [100, 250],\n",
    "        \"estimator__max_depth\": [20, None],\n",
    "        \"estimator__min_samples_split\": [2, 8, 10, 15],\n",
    "        \"estimator__min_samples_leaf\": [1, 3, 5],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Obtaining the selected features based on the full dataset\n",
    "selected_features_rf_binary, selected_params_rf_binary_full = rf_binary_features(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    features=features,\n",
    "    search_space=rf_search_space,\n",
    "    cv_splits=5,\n",
    "    class_weight=\"balanced\",\n",
    "    min_features_to_select=1,\n",
    "    GS_score=\"f1\",\n",
    "    GS_randomized=False,\n",
    "    GS_n_iter=10,\n",
    "    verbose=10,\n",
    ")\n",
    "\n",
    "print(f\"Number of selected features RF Binary: {len(selected_features_rf_binary)}\")\n",
    "print(f\"Selected features RF Binary: {selected_features_rf_binary}\")\n",
    "print(f\"Selected Parameters RF Binary {selected_params_rf_binary_full}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6578cc14",
   "metadata": {},
   "source": [
    "#### Training the optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4300e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the random forest search grid\n",
    "\n",
    " \n",
    "rf_search_space = [\n",
    "    {\n",
    "        \"rf__n_estimators\": [100, 250],\n",
    "        \"rf__max_depth\": [20, None],\n",
    "        \"rf__min_samples_split\": [2, 8, 15],\n",
    "        \"rf__min_samples_leaf\": [1, 3, 5],\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# Obtaining the performance estimate\n",
    "df_predicted_rf_binary, selected_params_rf_binary = rf_binary_performance(\n",
    "    df_train_list=df_train_list,\n",
    "    df_test_list=df_test_list,\n",
    "    y_var='DAM_binary_dmg',\n",
    "    features=selected_features_rf_binary,\n",
    "    search_space=rf_search_space,\n",
    "    stratK=True,\n",
    "    cv_splits=5,\n",
    "    class_weight=\"balanced\",\n",
    "    GS_score=\"f1\",\n",
    "    GS_randomized=False,\n",
    "    GS_n_iter=10,\n",
    "    verbose=10,\n",
    ")\n",
    "\n",
    "file_name = \"models\\\\output\\\\02\\\\selected_params_rf_binary.p\"\n",
    "path = os.path.join(cdir, file_name)\n",
    "pickle.dump(selected_params_rf_binary, open(path, \"wb\"))\n",
    "\n",
    "file_name = \"models\\\\output\\\\02\\\\df_predicted_rf_binary.csv\"\n",
    "path = os.path.join(cdir, file_name)\n",
    "df_predicted_rf_binary.to_csv(path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50f88b8",
   "metadata": {},
   "source": [
    "#### XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805ac9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "combined_input_data = combined_input_data[combined_input_data['DAM_perc_dmg'].notnull()]\n",
    "X = combined_input_data[features]\n",
    "y = combined_input_data[\"DAM_binary_dmg\"]\n",
    "\n",
    "# Setting the train and the test sets for obtaining performance estimate\n",
    "df_train_list, df_test_list = splitting_train_test(combined_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e2a65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the XGBoost search grid for full dataset\n",
    "xgb_search_space = [\n",
    "    {\n",
    "        \"estimator__learning_rate\": [0.1, 0.5, 1],\n",
    "        \"estimator__gamma\": [0.1, 0.5, 2],\n",
    "        \"estimator__max_depth\": [6, 8],\n",
    "        \"estimator__reg_lambda\": [0.001, 0.1, 1],\n",
    "        \"estimator__n_estimators\": [100, 200],\n",
    "        \"estimator__colsample_bytree\": [0.5, 0.7],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Obtaining the selected features based on the full dataset\n",
    "selected_features_xgb_binary, selected_params_xgb_binary_full = xgb_binary_features(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    features=features,\n",
    "    search_space=xgb_search_space,\n",
    "    objective=\"binary:hinge\",\n",
    "    cv_splits=5,\n",
    "    min_features_to_select=1,\n",
    "    GS_score=\"f1\",\n",
    "    GS_n_iter=50,\n",
    "    GS_randomized=True,\n",
    "    verbose=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef332289",
   "metadata": {},
   "source": [
    "#### Training the optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd92eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train_list, df_test_list = splitting_train_test(combined_input_data)\n",
    "\n",
    "selected_features_xgb_binary=selected_features_xgb_regr\n",
    "\n",
    "# Setting the XGBoost search grid\n",
    "xgb_search_space = [\n",
    "    {\n",
    "        \"xgb__learning_rate\": [0.1, 0.5, 1],\n",
    "        \"xgb__gamma\": [0.1, 0.5, 2],\n",
    "        \"xgb__max_depth\": [6, 8],\n",
    "        \"xgb__reg_lambda\": [0.001, 0.1, 1],\n",
    "        \"xgb__n_estimators\": [100, 200],\n",
    "        \"xgb__colsample_bytree\": [0.5, 0.7],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Obtaining the performance estimate\n",
    "df_predicted_xgb_binary, selected_params_xgb_binary = xgb_binary_performance(\n",
    "    df_train_list=df_train_list,\n",
    "    df_test_list=df_test_list,\n",
    "    y_var='DAM_binary_dmg',\n",
    "    features=selected_features_xgb_binary,\n",
    "    search_space=xgb_search_space,\n",
    "    stratK=True,\n",
    "    cv_splits=5,\n",
    "    objective=\"binary:hinge\",\n",
    "    GS_score=\"f1\",\n",
    "    GS_randomized=True,\n",
    "    GS_n_iter=50,\n",
    "    verbose=10,\n",
    ")\n",
    "\n",
    "file_name = \"models\\\\output\\\\02\\\\selected_params_xgb_binary.p\"\n",
    "path = os.path.join(cdir, file_name)\n",
    "pickle.dump(selected_params_xgb_binary, open(path, \"wb\"))\n",
    "\n",
    "file_name = \"models\\\\output\\\\02\\\\df_predicted_xgb_binary.csv\"\n",
    "path = os.path.join(cdir, file_name)\n",
    "df_predicted_xgb_binary.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eb2ae7",
   "metadata": {},
   "source": [
    "#### Base line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8be7191",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unweighted_random(y_train, y_test):\n",
    "    options = y_train.value_counts(normalize=True)\n",
    "    y_pred = random.choices(population=list(options.index), k=len(y_test))\n",
    "    return y_pred\n",
    "\n",
    "def weighted_random(y_train, y_test):\n",
    "    options = y_train.value_counts()\n",
    "    y_pred = random.choices(\n",
    "        population=list(options.index), weights=list(options.values), k=len(y_test)\n",
    "    )\n",
    "    return y_pred\n",
    "\n",
    "df_predicted_random = pd.DataFrame(columns=[\"typhoon\", \"actual\", \"predicted\"])\n",
    "\n",
    "for i in range(len(df_train_list)):\n",
    "\n",
    "    train = df_train_list[i]\n",
    "    test = df_test_list[i]\n",
    "\n",
    "    y_train = train[\"DAM_binary_dmg\"]\n",
    "    y_test = test[\"DAM_binary_dmg\"]\n",
    "\n",
    "    y_pred_test = unweighted_random(y_train, y_test)\n",
    "    df_predicted_temp = pd.DataFrame(\n",
    "        {\"typhoon\": test[\"typhoon\"], \"actual\": y_test, \"predicted\": y_pred_test}\n",
    "    )\n",
    "\n",
    "    df_predicted_random = pd.concat([df_predicted_random, df_predicted_temp])\n",
    "\n",
    "\n",
    "file_name = \"models\\\\output\\\\02\\\\df_predicted_random.csv\"\n",
    "path = os.path.join(cdir, file_name)\n",
    "df_predicted_random.to_csv(path, index=False)\n",
    "    \n",
    "df_predicted_random_weighted = pd.DataFrame(columns=[\"typhoon\", \"actual\", \"predicted\"])\n",
    "\n",
    "for i in range(len(df_train_list)):\n",
    "\n",
    "    train = df_train_list[i]\n",
    "    test = df_test_list[i]\n",
    "\n",
    "    y_train = train[\"DAM_binary_dmg\"]\n",
    "    y_test = test[\"DAM_binary_dmg\"]\n",
    "\n",
    "    y_pred_test = weighted_random(y_train, y_test)\n",
    "    df_predicted_temp = pd.DataFrame(\n",
    "        {\"typhoon\": test[\"typhoon\"], \"actual\": y_test, \"predicted\": y_pred_test}\n",
    "    )\n",
    "\n",
    "    df_predicted_random_weighted = pd.concat(\n",
    "        [df_predicted_random_weighted, df_predicted_temp]\n",
    "    )\n",
    "\n",
    "    \n",
    "file_name = \"models\\\\output\\\\02\\\\df_predicted_random_weighted.csv\"\n",
    "path = os.path.join(cdir, file_name)\n",
    "df_predicted_random_weighted.to_csv(path, index=False)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
