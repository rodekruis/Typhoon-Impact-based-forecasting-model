{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b6a1514",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "This note book explains the different steps in the machine learning model for the binary classfication model. First the model is trained on the full dataset to obtain the optimal features followed by hyper parameter tunning and model performance estimate using Nested Cross Validation.\n",
    "\n",
    "* Nested Cross Validation for\n",
    "    * Feature selection \n",
    "    * hyper parameter tunning \n",
    "* Performance metrics\n",
    "* Baseline Models\n",
    "\n",
    "### Binary Classification\n",
    "At the end of this section we will obtain  the optimal Binary Classification models and the performance estimates, \n",
    "for a 10% threshold. Two models are implemented: Random Forest Classifier, XGBoost Classifier. \n",
    "First, the model is trained on the full dataset to obtain the optimal features followed by a model \n",
    "that obtains the performance estimate using Nested Cross Validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcd382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import importlib\n",
    "\n",
    "from pathlib import Path\n",
    "# Third-party library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import openpyxl\n",
    "import geopandas as gpd\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedKFold,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    KFold,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    confusion_matrix,\n",
    "    make_scorer,\n",
    ")\n",
    "from sklearn.feature_selection import (\n",
    "    RFECV,\n",
    "    SelectKBest,\n",
    "    SequentialFeatureSelector,\n",
    "    RFE,\n",
    "    mutual_info_regression,\n",
    "    mutual_info_classif,\n",
    "    f_regression,\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# XGBoost imports\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier, XGBRegressor\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac563676",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def binary_damage_class(x):\n",
    "    damage = x[0]   \n",
    "    if damage > 10:\n",
    "        value = 1\n",
    "    else:\n",
    "        value = 0\n",
    "    return value\n",
    "\n",
    "\n",
    "def splitting_train_test(df):\n",
    "\n",
    "    # To save the train and test sets\n",
    "    df_train_list = []\n",
    "    df_test_list = []\n",
    "\n",
    "    # List of typhoons that are to be used as a test set \n",
    " \n",
    "    typhoons_with_impact_data=list(np.unique(df.typhoon))\n",
    "\n",
    "    for typhoon in typhoons_with_impact_data:\n",
    "        if len(df[df[\"typhoon\"] == typhoon]) >1:\n",
    "            df_train_list.append(df[df[\"typhoon\"] != typhoon])\n",
    "            df_test_list.append(df[df[\"typhoon\"] == typhoon])\n",
    "\n",
    "    return df_train_list, df_test_list\n",
    "\n",
    "def unweighted_random(y_train, y_test):\n",
    "    options = y_train.value_counts(normalize=True)\n",
    "    y_pred = random.choices(population=list(options.index), k=len(y_test))\n",
    "    return y_pred\n",
    "\n",
    "def weighted_random(y_train, y_test):\n",
    "    options = y_train.value_counts()\n",
    "    y_pred = random.choices(\n",
    "        population=list(options.index), weights=list(options.values), k=len(y_test)\n",
    "    )\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edfd2288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting directory\n",
    "\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = Path.cwd()\n",
    "\n",
    "wor_dir = current_dir / Path('../../')\n",
    "\n",
    "\n",
    "os.chdir(wor_dir)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09c601cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import functions\n",
    "from models.binary_classification.rf_binary import (rf_binary_features,rf_binary_performance,)\n",
    "from models.binary_classification.xgb_binary import (xgb_binary_features,xgb_binary_performance,)\n",
    "from models.regression.rf_regression import (rf_regression_features,rf_regression_performance,)\n",
    "from models.regression.xgb_regression import (xgb_regression_features,xgb_regression_performance,)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe30206",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "combined_input_data=pd.read_csv(\"./data/model_input/combined_input_data.csv\")\n",
    "\n",
    "combined_input_data[\"DAM_binary_dmg\"] = combined_input_data[[\"DAM_perc_dmg\"]].apply(binary_damage_class, axis=\"columns\")\n",
    "\n",
    "\n",
    "combined_input_data =combined_input_data.filter(['typhoon','HAZ_rainfall_Total', \n",
    "        'HAZ_rainfall_max_6h',\n",
    "        'HAZ_rainfall_max_24h',\n",
    "        'HAZ_v_max',\n",
    "        'HAZ_dis_track_min',\n",
    "        'GEN_landslide_per',\n",
    "        'GEN_stormsurge_per',\n",
    "        'GEN_Bu_p_inSSA', \n",
    "        'GEN_Bu_p_LS', \n",
    "        'GEN_Red_per_LSbldg',\n",
    "        'GEN_Or_per_LSblg', \n",
    "        'GEN_Yel_per_LSSAb', \n",
    "        'GEN_RED_per_SSAbldg',\n",
    "        'GEN_OR_per_SSAbldg',\n",
    "        'GEN_Yellow_per_LSbl',\n",
    "        'TOP_mean_slope',\n",
    "        'TOP_mean_elevation_m', \n",
    "        'TOP_ruggedness_stdev', \n",
    "        'TOP_mean_ruggedness',\n",
    "        'TOP_slope_stdev', \n",
    "        'VUL_poverty_perc',\n",
    "        'GEN_with_coast',\n",
    "        'GEN_coast_length', \n",
    "        'VUL_Housing_Units',\n",
    "        'VUL_StrongRoof_StrongWall', \n",
    "        'VUL_StrongRoof_LightWall',\n",
    "        'VUL_StrongRoof_SalvageWall', \n",
    "        'VUL_LightRoof_StrongWall',\n",
    "        'VUL_LightRoof_LightWall', \n",
    "        'VUL_LightRoof_SalvageWall',\n",
    "        'VUL_SalvagedRoof_StrongWall',\n",
    "        'VUL_SalvagedRoof_LightWall',\n",
    "        'VUL_SalvagedRoof_SalvageWall', \n",
    "        'VUL_vulnerable_groups',\n",
    "        'VUL_pantawid_pamilya_beneficiary',\n",
    "        'DAM_binary_dmg'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d29b0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "features =['HAZ_rainfall_Total', \n",
    "        'HAZ_rainfall_max_6h',\n",
    "        'HAZ_rainfall_max_24h',\n",
    "        'HAZ_v_max',\n",
    "        'HAZ_dis_track_min',\n",
    "        'GEN_landslide_per',\n",
    "        'GEN_stormsurge_per',\n",
    "        'GEN_Bu_p_inSSA', \n",
    "        'GEN_Bu_p_LS', \n",
    "        'GEN_Red_per_LSbldg',\n",
    "        'GEN_Or_per_LSblg', \n",
    "        'GEN_Yel_per_LSSAb', \n",
    "        'GEN_RED_per_SSAbldg',\n",
    "        'GEN_OR_per_SSAbldg',\n",
    "        'GEN_Yellow_per_LSbl',\n",
    "        'TOP_mean_slope',\n",
    "        'TOP_mean_elevation_m', \n",
    "        'TOP_ruggedness_stdev', \n",
    "        'TOP_mean_ruggedness',\n",
    "        'TOP_slope_stdev', \n",
    "        'VUL_poverty_perc',\n",
    "        'GEN_with_coast',\n",
    "        'GEN_coast_length', \n",
    "        'VUL_Housing_Units',\n",
    "        'VUL_StrongRoof_StrongWall', \n",
    "        'VUL_StrongRoof_LightWall',\n",
    "        'VUL_StrongRoof_SalvageWall', \n",
    "        'VUL_LightRoof_StrongWall',\n",
    "        'VUL_LightRoof_LightWall', \n",
    "        'VUL_LightRoof_SalvageWall',\n",
    "        'VUL_SalvagedRoof_StrongWall',\n",
    "        'VUL_SalvagedRoof_LightWall',\n",
    "        'VUL_SalvagedRoof_SalvageWall', \n",
    "        'VUL_vulnerable_groups',\n",
    "        'VUL_pantawid_pamilya_beneficiary']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e1f45",
   "metadata": {},
   "source": [
    "####  Random forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0a21a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_input_data.query(\"DAM_binary_dmg>0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6a4e712",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=combined_input_data.dropna()\n",
    " \n",
    "#combined_input_data = combined_input_data[combined_input_data['DAM_perc_dmg'].notnull()]\n",
    "X = df[features]\n",
    "y = df[\"DAM_binary_dmg\"]\n",
    "\n",
    "# Setting the train and the test sets for obtaining performance estimate\n",
    "df_train_list, df_test_list = splitting_train_test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65341361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the random forest search grid\n",
    "rf_search_space = [\n",
    "    {\n",
    "        \"estimator__n_estimators\": [100, 250],\n",
    "        \"estimator__max_depth\": [20, None],\n",
    "        \"estimator__min_samples_split\": [2, 8, 10, 15],\n",
    "        \"estimator__min_samples_leaf\": [1, 3, 5],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Obtaining the selected features based on the full dataset\n",
    "selected_features_rf_binary, selected_params_rf_binary_full = rf_binary_features(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    features=features,\n",
    "    search_space=rf_search_space,\n",
    "    cv_splits=5,\n",
    "    class_weight=\"balanced\",\n",
    "    min_features_to_select=1,\n",
    "    GS_score=\"f1\",\n",
    "    GS_randomized=False,\n",
    "    GS_n_iter=10,\n",
    "    verbose=10,\n",
    ")\n",
    "\n",
    "print(f\"Number of selected features RF Binary: {len(selected_features_rf_binary)}\")\n",
    "print(f\"Selected features RF Binary: {selected_features_rf_binary}\")\n",
    "print(f\"Selected Parameters RF Binary {selected_params_rf_binary_full}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "54689228",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_rf_binary=[\n",
    "    'HAZ_rainfall_Total',\n",
    "     'HAZ_rainfall_max_6h',\n",
    "     'HAZ_rainfall_max_24h',\n",
    "     'HAZ_v_max',\n",
    "     'HAZ_dis_track_min',\n",
    "     'GEN_landslide_per',\n",
    "     'GEN_stormsurge_per',\n",
    "     'TOP_mean_slope',\n",
    "     'TOP_mean_elevation_m',\n",
    "     'TOP_ruggedness_stdev',\n",
    "     'TOP_mean_ruggedness',\n",
    "     'TOP_slope_stdev',\n",
    "     'VUL_poverty_perc',\n",
    "     'GEN_coast_length',\n",
    "     'VUL_Housing_Units',\n",
    "     'VUL_StrongRoof_StrongWall',\n",
    "     'VUL_StrongRoof_SalvageWall',\n",
    "     'VUL_LightRoof_StrongWall',\n",
    "     'VUL_LightRoof_SalvageWall',\n",
    "     'VUL_SalvagedRoof_StrongWall',\n",
    "     'VUL_vulnerable_groups',\n",
    "     'VUL_pantawid_pamilya_beneficiary'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6578cc14",
   "metadata": {},
   "source": [
    "#### Training the optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8402f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_name = \"./models/output/v1/selected_params_rf_binary2.p\"\n",
    "\n",
    "pickle.dump(selected_params_rf_binary, open(file_name, \"wb\"))\n",
    "\n",
    "file_name = \"./models/output/v1/df_predicted_rf_binary2.csv\"\n",
    "\n",
    "df_predicted_rf_binary.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4300e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the random forest search grid\n",
    "\n",
    " \n",
    "rf_search_space = [\n",
    "    {\n",
    "        \"rf__n_estimators\": [500],\n",
    "        \"rf__max_depth\": [22],\n",
    "        \"rf__min_samples_split\": [2],\n",
    "        \"rf__min_samples_leaf\": [3]\n",
    "        \n",
    "    }\n",
    "]\n",
    "# Obtaining the performance estimate\n",
    "df_predicted_rf_binary, selected_params_rf_binary = rf_binary_performance(\n",
    "    df_train_list=df_train_list,\n",
    "    df_test_list=df_test_list,\n",
    "    y_var='DAM_binary_dmg',\n",
    "    features=selected_features_rf_binary,\n",
    "    search_space=rf_search_space,\n",
    "    stratK=True,\n",
    "    cv_splits=5,\n",
    "    class_weight=\"balanced\",\n",
    "    GS_score=\"f1\",\n",
    "    GS_randomized=False,\n",
    "    GS_n_iter=50,\n",
    "    verbose=10,\n",
    ")\n",
    "\n",
    "#n_samples / (n_classes * np.bincount(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50f88b8",
   "metadata": {},
   "source": [
    "#### XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "805ac9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "combined_input_data = combined_input_data[combined_input_data['DAM_binary_dmg'].notnull()]\n",
    "X = combined_input_data[features]\n",
    "y = combined_input_data[\"DAM_binary_dmg\"]\n",
    "\n",
    "# Setting the train and the test sets for obtaining performance estimate\n",
    "df_train_list, df_test_list = splitting_train_test(combined_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e2a65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the XGBoost search grid for full dataset\n",
    "xgb_search_space = [\n",
    "    {\n",
    "        \"estimator__learning_rate\": [0.1, 0.5, 1],\n",
    "        \"estimator__gamma\": [0.1, 0.5, 2],#0\n",
    "        \"estimator__max_depth\": [6, 8],\n",
    "        \"estimator__reg_lambda\": [0.001, 0.1, 1],\n",
    "        \"estimator__n_estimators\": [100, 200],\n",
    "        \"estimator__colsample_bytree\": [0.5, 0.7],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Obtaining the selected features based on the full dataset\n",
    "selected_features_xgb_binary, selected_params_xgb_binary_full = xgb_binary_features(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    features=features,\n",
    "    search_space=xgb_search_space,\n",
    "    objective=\"binary:hinge\",\n",
    "    cv_splits=5,\n",
    "    min_features_to_select=1,\n",
    "    GS_score=\"f1\",\n",
    "    GS_n_iter=50,\n",
    "    GS_randomized=True,\n",
    "    verbose=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08625759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef332289",
   "metadata": {},
   "source": [
    "#### Training the optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f0808c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_xgb_regr =[\n",
    "    'HAZ_v_max',\n",
    "    'HAZ_dis_track_min',\n",
    "    'VUL_StrongRoof_StrongWall',\n",
    "    'TOP_mean_elevation_m',\n",
    "    'HAZ_rainfall_max_6h',\n",
    "    'HAZ_rainfall_max_24h',\n",
    "    'HAZ_rainfall_Total',\n",
    "    'VUL_vulnerable_groups',\n",
    "    'VUL_pantawid_pamilya_beneficiary',\n",
    "    'VUL_StrongRoof_LightWall',\n",
    "    'VUL_poverty_perc',\n",
    "    'TOP_ruggedness_stdev',\n",
    "    'TOP_slope_stdev',\n",
    "    'TOP_mean_slope',\n",
    "    'GEN_coast_length',\n",
    "    'VUL_Housing_Units',\n",
    "    'GEN_stormsurge_per',\n",
    "    'GEN_landslide_per',\n",
    "    'TOP_mean_ruggedness',\n",
    "    'GEN_Yel_per_LSSAb',\n",
    "    'GEN_Yellow_per_LSbl',\n",
    "    'GEN_Red_per_LSbldg',\n",
    "    'GEN_with_coast',\n",
    "    'GEN_OR_per_SSAbldg',\n",
    "    'GEN_RED_per_SSAbldg',\n",
    "    'GEN_Or_per_LSblg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd92eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train_list, df_test_list = splitting_train_test(combined_input_data)\n",
    "\n",
    "selected_features_xgb_binary=selected_features_xgb_regr\n",
    "\n",
    "\n",
    "xgb_search_space = [\n",
    "    {\n",
    "        \"xgb__learning_rate\": [0.3], #0.03\n",
    "        \"xgb__gamma\": [0.1], #0\n",
    "        \"xgb__max_depth\": [6], #6\n",
    "        \"xgb__reg_lambda\": [0.001],\n",
    "        \"xgb__n_estimators\": [50],\n",
    "        \"xgb__colsample_bytree\": [0.7],#1\n",
    "    }\n",
    "]\n",
    "# Obtaining the performance estimate\n",
    "df_predicted_xgb_binary, selected_params_xgb_binary = xgb_binary_performance(\n",
    "    df_train_list=df_train_list,\n",
    "    df_test_list=df_test_list,\n",
    "    y_var='DAM_binary_dmg',\n",
    "    features=selected_features_xgb_binary,\n",
    "    search_space=xgb_search_space,\n",
    "    stratK=True,\n",
    "    cv_splits=5,\n",
    "    objective=\"binary:hinge\",\n",
    "    GS_score=\"f1\",\n",
    "    GS_randomized=True,\n",
    "    GS_n_iter=100,\n",
    "    verbose=10,\n",
    ")\n",
    "\n",
    "file_name = \"./models/output/v1/selected_params_xgb_binary.p\"\n",
    "\n",
    "pickle.dump(selected_params_xgb_binary, open(file_name, \"wb\"))\n",
    "\n",
    "file_name = \"./models/output/v1/df_predicted_xgb_binary.csv\"\n",
    "\n",
    "df_predicted_xgb_binary.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eb2ae7",
   "metadata": {},
   "source": [
    "#### Base line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8be7191",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unweighted_random(y_train, y_test):\n",
    "    options = y_train.value_counts(normalize=True)\n",
    "    y_pred = random.choices(population=list(options.index), k=len(y_test))\n",
    "    return y_pred\n",
    "\n",
    "def weighted_random(y_train, y_test):\n",
    "    options = y_train.value_counts()\n",
    "    y_pred = random.choices(\n",
    "        population=list(options.index), weights=list(options.values), k=len(y_test)\n",
    "    )\n",
    "    return y_pred\n",
    "\n",
    "df_predicted_random = pd.DataFrame(columns=[\"typhoon\", \"actual\", \"predicted\"])\n",
    "\n",
    "for i in range(len(df_train_list)):\n",
    "\n",
    "    train = df_train_list[i]\n",
    "    test = df_test_list[i]\n",
    "\n",
    "    y_train = train[\"DAM_binary_dmg\"]\n",
    "    y_test = test[\"DAM_binary_dmg\"]\n",
    "\n",
    "    y_pred_test = unweighted_random(y_train, y_test)\n",
    "    df_predicted_temp = pd.DataFrame(\n",
    "        {\"typhoon\": test[\"typhoon\"], \"actual\": y_test, \"predicted\": y_pred_test}\n",
    "    )\n",
    "\n",
    "    df_predicted_random = pd.concat([df_predicted_random, df_predicted_temp])\n",
    "\n",
    "\n",
    "file_name = \"./models/output/v1/df_predicted_random.csv\"\n",
    "\n",
    "df_predicted_random.to_csv(file_name, index=False)\n",
    "    \n",
    "df_predicted_random_weighted = pd.DataFrame(columns=[\"typhoon\", \"actual\", \"predicted\"])\n",
    "\n",
    "for i in range(len(df_train_list)):\n",
    "\n",
    "    train = df_train_list[i]\n",
    "    test = df_test_list[i]\n",
    "\n",
    "    y_train = train[\"DAM_binary_dmg\"]\n",
    "    y_test = test[\"DAM_binary_dmg\"]\n",
    "\n",
    "    y_pred_test = weighted_random(y_train, y_test)\n",
    "    df_predicted_temp = pd.DataFrame(\n",
    "        {\"typhoon\": test[\"typhoon\"], \"actual\": y_test, \"predicted\": y_pred_test}\n",
    "    )\n",
    "\n",
    "    df_predicted_random_weighted = pd.concat(\n",
    "        [df_predicted_random_weighted, df_predicted_temp]\n",
    "    )\n",
    "\n",
    "    \n",
    "file_name = \"./models/output/v1/df_predicted_random_weighted.csv\"\n",
    "\n",
    "df_predicted_random_weighted.to_csv(file_name, index=False)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climada_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
